#!/usr/bin/env python3
"""
Text-to-Speech Converter for Vietnamese Book
Using VieNeu-TTS model from Hugging Face

Usage:
    python text_to_audio.py --input <path_to_markdown> --output <output_folder>

Requirements:
    pip install transformers torch accelerate soundfile librosa tqdm
"""

import os
import re
import argparse
import torch
import soundfile as sf
import librosa
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForTextToSpeech
from pathlib import Path

# C·∫•u h√¨nh
MODEL_NAME = "pnnbao-ump/VieNeu-TTS"
OUTPUT_SAMPLE_RATE = 22050
MAX_CHARS_PER_CHUNK = 200  # Gi·ªõi h·∫°n k√Ω t·ª± cho m·ªói chunk ƒë·ªÉ tr√°nh OOM

def clean_text_for_tts(text):
    """L√†m s·∫°ch text, lo·∫°i b·ªè markdown v√† k√Ω t·ª± ƒë·∫∑c bi·ªát"""
    # Lo·∫°i b·ªè header markdown (#, ##, ###)
    text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
    # Lo·∫°i b·ªè bold/italic (*, **, _)
    text = re.sub(r'\*\*|\*|_', '', text)
    # Lo·∫°i b·ªè links [text](url)
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    # Lo·∫°i b·ªè code blocks
    text = re.sub(r'```[\s\S]*?```', '', text)
    # Lo·∫°i b·ªè inline code
    text = re.sub(r'`([^`]+)`', r'\1', text)
    # Lo·∫°i b·ªè blockquotes
    text = re.sub(r'^>\s*', '', text, flags=re.MULTILINE)
    # Lo·∫°i b·ªè horizontal rules
    text = re.sub(r'^[-*_]{3,}$', '', text, flags=re.MULTILINE)
    # Lo·∫°i b·ªè b·∫£ng (gi·ªØ l·∫°i n·ªôi dung)
    text = re.sub(r'\|[|\-\s]+\|', '', text)
    # Lo·∫°i b·ªè d√≤ng tr·ªëng th·ª´a
    text = re.sub(r'\n{3,}', '\n\n', text)
    # Thay th·∫ø c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát b·∫±ng kho·∫£ng tr·∫Øng
    text = re.sub(r'[^\w\s.,!?;:()\'"-]', ' ', text)
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def split_text_into_chunks(text, max_chars=MAX_CHARS_PER_CHUNK):
    """Chia text th√†nh c√°c chunks nh·ªè ƒë·ªÉ x·ª≠ l√Ω"""
    # T√°ch theo c√¢u
    sentences = re.split(r'(?<=[.!?])\s+', text)

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) < max_chars:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def load_model(model_name):
    """Load VieNeu-TTS model"""
    print(f"üîÑ ƒêang t·∫£i model: {model_name}")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForTextToSpeech.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )

    # Move to GPU if available
    if torch.cuda.is_available():
        model = model.to("cuda")
        print("‚úÖ S·ª≠ d·ª•ng GPU")
    else:
        print("‚úÖ S·ª≠ d·ª•ng CPU")

    return tokenizer, model

def text_to_speech(tokenizer, model, text, output_path):
    """Chuy·ªÉn ƒë·ªïi text th√†nh audio"""
    # Chu·∫©n b·ªã input
    inputs = tokenizer(text, return_tensors="pt")

    # Move to same device as model
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    # Generate speech
    with torch.no_grad():
        speech = model.generate_speech(
            inputs["input_ids"],
            attention_mask=inputs.get("attention_mask")
        )

    # Convert to numpy and save
    audio = speech.cpu().numpy()

    # Resample to desired sample rate if needed
    if model.config.audio_encoder_sample_rate != OUTPUT_SAMPLE_RATE:
        audio = librosa.resample(
            audio,
            orig_sr=model.config.audio_encoder_sample_rate,
            target_sr=OUTPUT_SAMPLE_RATE
        )

    sf.write(output_path, audio, OUTPUT_SAMPLE_RATE)
    return len(audio) / OUTPUT_SAMPLE_RATE  # Return duration in seconds

def combine_audio_files(audio_files, output_path):
    """K·∫øt h·ª£p nhi·ªÅu file audio th√†nh m·ªôt"""
    print("üéµ ƒêang k·∫øt h·ª£p c√°c file audio...")

    combined_audio = None
    for file_path in tqdm(audio_files):
        audio, sr = librosa.load(file_path, sr=OUTPUT_SAMPLE_RATE)
        if combined_audio is None:
            combined_audio = audio
        else:
            combined_audio = np.concatenate([combined_audio, audio])

    sf.write(output_path, combined_audio, OUTPUT_SAMPLE_RATE)
    print(f("‚úÖ ƒê√£ l∆∞u file audio t·ªïng h·ª£p: {output_path}")
    print(f("   Th·ªùi l∆∞·ª£ng: {len(combined_audio) / OUTPUT_SAMPLE_RATE:.2f} gi√¢y")

import numpy as np

def main():
    parser = argparse.ArgumentParser(
        description="Chuy·ªÉn ƒë·ªïi s√°ch ti·∫øng Vi·ªát th√†nh audio s·ª≠ d·ª•ng VieNeu-TTS"
    )
    parser.add_argument(
        "--input", "-i",
        default="/Users/mac/tools/AIPM/ebooks/00-Full-Book/INSPIRED-Full-Book.md",
        help="ƒê∆∞·ªùng d·∫´n file markdown ƒë·∫ßu v√†o"
    )
    parser.add_argument(
        "--output", "-o",
        default="/Users/mac/tools/AIPM/ebooks/00-Full-Book/INSPIRED-Full-Book.wav",
        help="ƒê∆∞·ªùng d·∫´n file audio ƒë·∫ßu ra"
    )
    parser.add_argument(
        "--temp-dir", "-t",
        default="/Users/mac/tools/AIPM/ebooks/00-Full-Book/temp_audio",
        help="Th∆∞ m·ª•c t·∫°m cho c√°c chunk audio"
    )
    parser.add_argument(
        "--chunk-size", "-c",
        type=int,
        default=MAX_CHARS_PER_CHUNK,
        help="S·ªë k√Ω t·ª± t·ªëi ƒëa cho m·ªói chunk"
    )

    args = parser.parse_args()

    # T·∫°o th∆∞ m·ª•c t·∫°m
    temp_dir = Path(args.temp_dir)
    temp_dir.mkdir(parents=True, exist_ok=True)

    # ƒê·ªçc file text
    print(f"üìñ ƒêang ƒë·ªçc file: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        raw_text = f.read()
    print(f"   T·ªïng s·ªë k√Ω t·ª±: {len(raw_text)}")

    # L√†m s·∫°ch text
    print("üßπ ƒêang l√†m s·∫°ch text...")
    clean_text = clean_text_for_tts(raw_text)
    print(f"   K√Ω t·ª± sau khi l√†m s·∫°ch: {len(clean_text)}")

    # Chia th√†nh chunks
    print("‚úÇÔ∏è ƒêang chia text th√†nh chunks...")
    chunks = split_text_into_chunks(clean_text, args.chunk_size)
    print(f"   T·ªïng s·ªë chunks: {len(chunks)}")

    # Load model
    tokenizer, model = load_model(MODEL_NAME)

    # T·∫°o audio cho m·ªói chunk
    audio_files = []
    print("üéôÔ∏è ƒêang t·∫°o audio...")

    for i, chunk in enumerate(tqdm(chunks)):
        if not chunk.strip():
            continue

        output_file = temp_dir / f"chunk_{i:04d}.wav"
        try:
            duration = text_to_speech(
                tokenizer, model,
                chunk.strip(),
                str(output_file)
            )
            audio_files.append(output_file)
            tqdm.write(f"   Chunk {i+1}/{len(chunks)}: {duration:.2f}s")
        except Exception as e:
            tqdm.write(f"‚ùå L·ªói ·ªü chunk {i}: {e}")

    # K·∫øt h·ª£p audio
    if audio_files:
        combine_audio_files(audio_files, args.output)
    else:
        print("‚ùå Kh√¥ng c√≥ file audio n√†o ƒë∆∞·ª£c t·∫°o")

    # Cleanup temp files (optional)
    print(f"\nüóëÔ∏è ƒêang d·ªçn d·∫πp files t·∫°m...")
    for f in audio_files:
        f.unlink()
    temp_dir.rmdir()

    print("\n‚ú® Ho√†n th√†nh!")

if __name__ == "__main__":
    main()
